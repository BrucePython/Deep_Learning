{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一. 特征工程\n",
    "    \n",
    "- 特征工程是使用专业背景知识和技巧处理数据，使得特征能在机器学习算法上发挥更好的作用的过程\n",
    "\n",
    "- 意义：会直接影响机器学习的效果\n",
    "\n",
    "\n",
    "## 1. 特征抽取：\n",
    "\n",
    "- 包括将任意数据（如文本或图像）转换为可用于机器学习的数字特征，特征值化是为了计算机更好的去理解数据\n",
    "- 特征提取API：\n",
    "    - sklearn.feature_extraction\n",
    "\n",
    "\n",
    "### 1.1 字典特征提取\n",
    "     \n",
    "- 特征离散化\n",
    "\n",
    "### 对特征当中有类别的信息做（one-hot编码）处理 —>（sparse=False）\n",
    "    \n",
    "- one-hot编码：防止信息的歧义\n",
    "    [[ 0. 1. 0. 100.] [ 1. 0. 0. 60.] [ 0. 0. 1. 30.]]\n",
    "     \n",
    "    sklearn.feature_extraction.DictVectorizer(sparse=True,…)\n",
    "        \n",
    "        DictVec = DictVectorizer()    # 实例化一个字典特征对象\n",
    "        \n",
    "        DictVec.fit_transform(X) X:字典或者包含字典的迭代器返回值：返回sparse矩阵（稀疏矩阵）\n",
    "        DictVec.inverse_transform(X) X:array数组或者sparse矩阵 返回值:转换之前数据格式\n",
    "        DictVec.get_feature_names() 返回类别名称\n",
    "\n",
    "\n",
    "## 1.2 文本特征提取\n",
    "    \n",
    "    作用：对文本数据进行特征值化，两种方式转换：\n",
    "    \n",
    "    方法一：\n",
    "        sklearn.feature_extraction.text.CountVectorizer(stop_words=[])\n",
    "            返回词频矩阵\n",
    "        CountVectorizer.fit_transform(X) X:文本或者包含文本字符串的可迭代对象 返回值：返回sparse矩阵\n",
    "        CountVectorizer.inverse_transform(X) X:array数组或者sparse矩阵 返回值:转换之前数据格\n",
    "        CountVectorizer.get_feature_names() 返回值:单词列表\n",
    "    \n",
    "    方法二：\n",
    "        Tf-idf文本特征提取：\n",
    "            用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度\n",
    "            \n",
    "            tfidf = tf * idf = 重要性\n",
    "            tf：词频（term frequency）指的是某一个给定的词语在该文件中出现的频率\n",
    "            idf：由总文件数目除以包含该词语的文件的数目，再将得到的商取以10为底的对数得到\n",
    "        \n",
    "        例：文章1有100个词，“非常”这个词出现了5词，则tf为：5/100=0.05\n",
    "            包含文章1的文件集中，有1千万篇文章，1万个文章里面都出现了“非常”这个词，则idf为：lg(10m/10k)= 3\n",
    "            tfidf = tf * idf： = 0.05*3 = 0.15\n",
    "            “非常”这个词的重要性为0.15\n",
    "        \n",
    "        sklearn.feature_extraction.text.TfidfVectorizer\n",
    "        \n",
    "        对每篇文章的重要性排序，找到每篇前N个重要的词；分类机器学习算法进行文章分类中前期数据处理方式\n",
    "        \n",
    "    CountVectorizer统计单词出现的次数，基本不适用\n",
    "    TfidfVectorizer统计单词的重要性\n",
    "    DictVectorizer 字典特征处理，转成one-hot编码\n",
    "\n",
    "\n",
    "\n",
    "# 2. 特征预处理\n",
    "    \n",
    "    通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程，防止特征的数值影响结果\n",
    "    无量纲化：\n",
    "        归一化\n",
    "        标准化\n",
    "    \n",
    "## 2.1 归一化\n",
    "    \n",
    "    公式：\n",
    "        X' = (x-min)/(max-min)\n",
    "        X\" = X'*(mx-mi)+mi\n",
    "        作用于每一列，max为一列的最大值，min为一列的最小值,那么X’’为最终结果，mx，mi分别为指定区间值默认mx为1,mi为0\n",
    "        \n",
    "    API:\n",
    "        sklearn.preprocessing.MinMaxScaler (feature_range=(0,1)… )\n",
    "        \n",
    "        # 实例化一个MinMaxScalar对象，用对象调用方法\n",
    "        MinMaxScalar.fit_transform(X)\n",
    "        X:numpy array格式的数据[n_samples,n_features]\n",
    "        返回值：转换后的形状相同的array\n",
    "    \n",
    "    缺点：\n",
    "        异常点不好处理，故弃用\n",
    " \n",
    " \n",
    "## 2.2 标准化\n",
    "    \n",
    "    公式：\n",
    "        X' = (x-mean) - σ\n",
    "        作用于每一列，mean为平均值，σ为标准差\n",
    "        \n",
    "    API:\n",
    "        sklearn.preprocessing.StandardScaler( )\n",
    "        处理之后每列来说所有数据都聚集在均值0附近标准差差为1\n",
    "        \n",
    "        StandardScaler.fit_transform(X)\n",
    "        X:numpy array格式的数据[n_samples,n_features]\n",
    "        返回值：转换后的形状相同的array\n",
    "        \n",
    "    作用：\n",
    "        每列特征数据都聚集在均值0， 标准差为1范围\n",
    "        \n",
    "    优点：\n",
    "        在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。\n",
    "  \n",
    "\n",
    "\n",
    "# 3. 降维\n",
    "    \n",
    "    降维是指在某些限定条件下，降低随机变量(特征)个数，得到一组“不相关”主变量的过程。\n",
    "    \n",
    "    正是因为在进行训练的时候，我们都是使用特征进行学习。如果特征本身存在问题或者特征之间相关性较强，对于算法学习预测会影响较大\n",
    "    \n",
    "    相关特征(correlated feature)：湿度与降雨量之间的相关\n",
    "    \n",
    "    降维的两种方法：特征选择、主成分分析\n",
    "      \n",
    "     \n",
    "##  3.1 特征选择\n",
    "* 定义：\n",
    "    数据中包含冗余或无关变量（或称特征、属性、指标等），旨在从原有特征中找出主要特征。\n",
    "    \n",
    "* 三种方法：\n",
    "* Filter(过滤式)：主要探究特征本身特点、特征与特征和目标值之间关联\n",
    "    * 方差选择法：低方差特征过滤\n",
    "    * 相关系数\n",
    "* Embedded (嵌入式)：算法自动选择特征（特征与目标值之间的关联）\n",
    "    * 决策树:信息熵、信息增益\n",
    "            正则化：L1、L2\n",
    "            深度学习：卷积等\n",
    "* Wrapper (包裹式)\n",
    "\n",
    "\n",
    "### 3.1.1 过滤式\n",
    "* Filter(过滤式)：主要**探究特征本身特点**、**特征与特征**和目标值之间关联\n",
    "    * 方差选择法：低方差特征过滤\n",
    "        * 方差很小：所有样本的某个特征值基本上一样\n",
    "        * 方差很大：所有样本的某个特征值的差异性比较大\n",
    "        * 缺点：不太好选择这个方差的值，作为明显一些特征处理\n",
    "        \n",
    "        * API：删除所有低方差特征\n",
    "        sklearn.feature_selection.VarianceThreshold(threshold = 0.0)\n",
    "        默认把方差为0的特征，删除掉\n",
    "        Variance.fit_transform(X)\n",
    "        X:numpy array格式的数据[n_samples,n_features]\n",
    "        返回值：训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。\n",
    "        \n",
    "    * 相关系数：皮尔逊相关系数（衡量两个特征之间的相关性）\n",
    "        * 系数的值：[-1, 1]\n",
    "        * |r|<0.4为低度相关；0.4≤|r|<0.7为显著性相关；0.7≤|r|<1为高度线性相关\n",
    "        * 相关的特征必须做一些相应的处理（删掉其中一些，合成一个新的）\n",
    "        * API：\n",
    "        from scipy.stats import pearsonr\n",
    "        x : (N,) array_like\n",
    "        y : (N,) array_like Returns: (Pearson’s correlation coefficient, p-value)\n",
    "  \n",
    "\n",
    "## 3.2 主成分分析（可以理解一种特征提取的方式）\n",
    "* 高维数据转化为低维数据的过程，在此过程中**可能会舍弃原有数据、创造新的变量（特征）**\n",
    "* 场景：\n",
    "    * 特征数量非常大的时候（上百个特征）：PCA去压缩，相关的、冗余的信息，防止高维度特征的计算量大\n",
    "    * 创造新的变量（新的特征）：revenue与指标total_expense压缩成一个特征\n",
    "    * 'revenue', 'total_expense'这两个指标合成到一个    新的指标（特征）。data[['revenue', 'total_expense']]\n",
    "\n",
    "* API:\n",
    "    klearn.decomposition.PCA(n_components=None)\n",
    "        将数据分解为较低维数空间\n",
    "        n_components:\n",
    "        小数：表示保留百分之多少的信息，一般使用小数形式\n",
    "        整数：减少到多少特征（不知道损失了多少特征）\n",
    "        PCA.fit_transform(X) X:numpy array格式的数据 [n_samples,n_features]\n",
    "        返回值：转换后指定维度的array\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集划分成训练集和测试集（73开）：\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y , test_size=小数)\n",
    "    \n",
    "    x：数据集的特征值\n",
    "    y：数据集的目标值\n",
    "    train：训练集\n",
    "    test：测试集\n",
    "    test_size 测试集的大小，一般为float，如0.3\n",
    "    random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。\n",
    "    return ，测试集特征值(x_train)，训练集特征值(x_test)，训练集目标值(y_train)，测试集目标值(y_test)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 转换器\n",
    "\n",
    "    特征工程的接口称之为转换器，其中转换器调用有3种形式：fit_transform ,fit, transform\n",
    "    fit_transform = fit + transform\n",
    "    fit_transform(a):标准化：以自己a的平均值标准差转换自己\n",
    "    fit(a):计算a的平均值和标准差，transform(b)：以a的平均值和标准差去转换b\n",
    "    适用于特种选择、主成分分析，特种抽取\n",
    "    \n",
    "    【训练数据】的两种方法：\n",
    "        1. 先用fit获取【训练数据】的均值和标准差，transform使用该均值和标准差，进行无量化\n",
    "        2. 可直接使用fit_transform.\n",
    "        ps: fit_transform 【仅限】训练数据使用\n",
    "     \n",
    "    【测试数据】的一种方法：\n",
    "        # 先用fit获取【测试数据】的均值和标准差，transform使用该均值和标准差，进行无量化\n",
    "        一般用训练集的均值和方差，用于测试数据。transform使用该均值和标准差，进行无量化\n",
    "\n",
    "### 除了文本分类处理之外，其他的数据集若不以【历史数据】做参看，统统都用fit_transform转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 估计器\n",
    "    \n",
    "* 实现了大部分的sklearn机器学习算法，所以大部分的算法都可以称为估计器\n",
    "\n",
    "* 流程：\n",
    "    0. 划分训练（测试）集的特征值和目标值\n",
    "    1. 训练：fit(x_train,y_train)\n",
    "    2. 预测结果：y_predict = pridict(x_test)\n",
    "    3. 结果准确率： score = score(x_test, y_test)\n",
    "    \n",
    "    **用预测的结果（测试集特征值的结果），跟真实的结果（测试集目标值的结果）比对，判断算法的准确度的高低**\n",
    "    \n",
    "\n",
    "* 在sklearn中，估计器(estimator)是一个重要的角色，是一类实现了算法的API：\n",
    "        \n",
    "    * 用于分类的估计器：\n",
    "        sklearn.neighbors k-近邻算法\n",
    "        sklearn.naive_bayes 贝叶斯\n",
    "        sklearn.linear_model.LogisticRegression 逻辑回归\n",
    "        sklearn.tree 决策树与随机森林\n",
    "    * 用于回归的估计器：\n",
    "        sklearn.linear_model.LinearRegression 线性回归\n",
    "        sklearn.linear_model.Ridge 岭回归\n",
    "    * 用于无监督学习的估计器\n",
    "        sklearn.cluster.KMeans 聚类\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一. K-近邻算法\n",
    "\n",
    "定义：某样本在特征空间中的**k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别**，则该样本也属于这个类别。\n",
    "    \n",
    "    k值（邻居数）会影响最终结果:\n",
    "        k值取很小：容易受到异常点的影响\n",
    "        k值取很大：受到样本均衡的问题\n",
    "  \n",
    "    距离：欧氏距离\n",
    "  \n",
    "    算法API：\n",
    "* sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')\n",
    "* n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数\n",
    "* algorithm：{‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选用于计算最近邻居的算法：\n",
    "    ‘ball_tree’将会使用 BallTree，\n",
    "    ‘kd_tree’将使用 KDTree，\n",
    "    ‘auto’将尝试根据传递给fit方法的值来决定最合适的算法。 (不同实现方式影响效率)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型选择与调优\n",
    "    \n",
    "## 交叉验证：\n",
    "\n",
    "**为了让被评估的模型更加准确可信，为了看一个参数在这个数据集当中综合表现情况**\n",
    "\n",
    "* 将训练集分成：训练集+验证集        通常采取10折交叉验证\n",
    "  \n",
    "* 超参数：有很多参数是需要手动指定的（如k-近邻算法中的K值），这种叫超参数\n",
    "\n",
    "\n",
    "* **每组超参数都采用交叉验证**来进行评估。\n",
    "\n",
    "## 网格搜索估计器\n",
    "\n",
    "* 使用网格搜索和交叉验证找到合适的参数\n",
    "* API：\n",
    "    sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)\n",
    "    对估计器的指定参数值进行详尽搜索\n",
    "    estimator：估计器对象\n",
    "    param_grid：估计器对象的参数（可多数）(dict){“n_neighbors”:[1,3,5]}\n",
    "    cv：指定几折交叉验证\n",
    "    fit：输入训练数据\n",
    "    score：准确率\n",
    "    结果分析：\n",
    "    bestscore:在交叉验证中验证的最好结果_\n",
    "    bestestimator：最好的参数模型\n",
    "    cvresults:每次交叉验证后的验证集准确率结果和训练集准确率结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二. 朴素贝叶斯分类方法\n",
    "    \n",
    "    根据不同类别在同一个事件中的占比，决定该事件的分类，经常用于文本分类\n",
    "    \n",
    "##  概率基础 -- 条件概率与联合概率：\n",
    "\n",
    "* 联合概率：包含多个条件，且所有条件同时成立的概率 \n",
    "    * 记作：P(A,B)\n",
    "    * 特性：P(A, B) = P(A)P(B)\n",
    "    * 例：职业是程序员并且体重匀称：\n",
    "    p(程序员, 匀称) =  P(程序员)P(匀称) =3/7*(4/7) = 12/49\n",
    "    \n",
    "* 条件概率：就是事件A在另外一个事件B已经发生条件下的发生概率\n",
    "    * 记作：P(A|B)\n",
    "    * 特性：P(A1,A2|B) = P(A1|B)P(A2|B)\n",
    "    * 注意：此条件概率的成立，是由于A1,A2相互独立的结果(记忆)\n",
    "    \n",
    "    * 例：\n",
    "    1.在女生喜欢的条件下，职业是程序员的概率？\n",
    "    P(程序员|喜欢)=2/4=1/2\n",
    "    2. 在女神喜欢的条件下，职业是产品，体重是超重的概率？\n",
    "    P(产品, 超重|喜欢) = P(产品|喜欢)P(超重|喜欢)=1/2 *  1/4 = 1/8\n",
    "    \n",
    "## 贝叶斯公式\n",
    "    \n",
    "    P(C|W) = P(W|C)P(C) / P(W)\n",
    "    w为给定文档的特征值（频数统计，预测文档提供），C为文档类别\n",
    "    \n",
    "    由于篇文章w是由若干个词F1，F2，F3...组成\n",
    "    故公式可变为：\n",
    "    \n",
    "    P(C|F1,F2...) = P(F1,F2...|C)P(C) / P(F1,F2...)\n",
    "    P(F1,F2...|C) = P(F1|C)P(F2|C)...\n",
    "    \n",
    "    P(C)：某个文档类别的概率(某文档类别数／总文档数量)\n",
    "    P(W│C)：给定类别下特征（被预测文档中出现的词）的概率\n",
    "    计算方法：P(F1│C)=Ni/N （训练文档中去计算）\n",
    "            Ni为【该F1词】在【C类别所有文档】中出现的【次数】\n",
    "            N为所属类别C下的文档【所有词】出现的【次数和】\n",
    "    P(F1,F2,…) 预测文档中每个词的概率\n",
    "    \n",
    "    \n",
    "## 拉普拉斯平滑系数\n",
    "\n",
    "* 目的：防止计算出的分类概率为0\n",
    "    * P(F1|C) = (Ni+α)/(N+αm)\n",
    "    * α为指定的系数，一般为1；\n",
    "    * m为训练文档中统计出来的特征词数量\n",
    "\n",
    "## API\n",
    "\n",
    "    sklearn.naive_bayes.MultinomialNB(alpha = 1.0)\n",
    "    朴素贝叶斯分类\n",
    "    alpha：拉普拉斯平滑系数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三. 决策树分类算法\n",
    "\n",
    "算法必须通过数字分类\n",
    "\n",
    "- 决策树的划分：信息熵，信息增益\n",
    "    - 给出某个条件之后，对结果的不确定性（信息熵）消除得越多，则表明该条件带来的信息量越大，所以应该放在第一位\n",
    "    \n",
    "## 信息熵的定义：\n",
    "\n",
    "    - 信息不确定度\n",
    "    - 计算不确定性：H越大，不确定性也就越大\n",
    "    - H = -∑Pi*log2(Pi)  \n",
    "    - H的专业术语称之为信息熵，单位为比特。\n",
    "    - 负号：防止H变成负数\n",
    "    \n",
    "当这32支球队夺冠的几率相同时，对应的信息熵等于5比特\n",
    "只要概率发生任意变化，信息熵都比5比特小\n",
    "\n",
    "- 总结：\n",
    "    信息和消除不确定性是相联系的\n",
    "    信息量越大，不确定性越低\n",
    "    \n",
    "当我们得到的额外信息（球队历史比赛情况等等）越多的话，那么我们猜测的代价越小（猜测的不确定性减小）\n",
    "\n",
    "\n",
    "## 信息增益\n",
    "\n",
    "是决策分类算法的依据：得知某个特征，对总的信息熵减少的大小**减少越大：这个特征在建立树的时候越重要**\n",
    "\n",
    "- 公式：\n",
    "    特征A对训练数据集D的【信息增益g(D,A)】,定义为集合D的【信息熵H(D)】与特征A给定条件下D的信息【条件熵】H(D|A)之差，即公式为\n",
    "    g(D,A) = H(D) - H(D|A)\n",
    "    \n",
    "    - 条件熵计算公式：\n",
    "        H(D|A) = \n",
    "        \n",
    "- 例：（信息熵的计算是根据每个结果的占比）\n",
    "    * H(D) = -[(6/15)log(6/15)+(9/15)log(9/15)] = 0.971\n",
    "    * g(D,年龄) = 0.971 - [(5/15)H(青年)+(5/15)H(中年)+(5/15)H(老年)]\n",
    "    * H(青年) = -[(3/5)log(3/5)+(2/5)log(2/5)]\n",
    "    * H(中年) = -[(3/5)log(3/5)+(2/5)log(2/5)]\n",
    "    * H(老年) = -[(4/5)log(4/5)+(1/5)log(1/5)]\n",
    "\n",
    "## 决策树的三种算法实现\n",
    "\n",
    "    ID3\n",
    "    信息增益 最大的准则\n",
    "    C4.5\n",
    "    信息增益比 最大的准则\n",
    "    CART\n",
    "    分类树: 基尼系数 最小的准则 在sklearn中可以选择划分的默认原则\n",
    "    优势：划分更加细致（从后面例子的树显示来理解）\n",
    "\n",
    "### 决策树分类器API\n",
    "\n",
    "sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)\n",
    "\n",
    "- criterion:默认是’gini’系数，也可以选择信息增益的熵’entropy’\n",
    "- max_depth:树的深度大小\n",
    "- random_state:随机数种子\n",
    "- 其中会有些超参数：max_depth:树的深度大小\n",
    "- 其它超参数我们会结合随机森林讲解\n",
    "\n",
    "## 集成学习方法\n",
    "    \n",
    "    生成多个分类器/模型，最终结果多个分类器一起决定\n",
    "\n",
    "## 随机森林\n",
    "    \n",
    "   - 用N来表示训练用例（样本）的个数，M表示特征数目。\n",
    "    1. 一次随机选出一个样本，重复N次， （有可能出现重复的样本）\n",
    "    2. 随机去选出m个特征, m <<M，建立决策\n",
    "  \n",
    "### 随机森林分类器API:\n",
    "   sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)\n",
    "\n",
    "\n",
    "- n_estimators：integer，optional（default = 10）森林里的树木数量120,200,300,500,800,1200\n",
    "- criteria：string，可选（default =“gini”）分割特征的测量方法\n",
    "- max_depth：integer或None，可选（默认=无）树的最大深度 5,8,15,25,30\n",
    "- max_features=\"auto”,每个决策树的最大特征数量\n",
    "    1. If \"auto\", then max_features=sqrt(n_features).\n",
    "    2. If \"sqrt\", then max_features=sqrt(n_features) (same as \"auto\").\n",
    "    3. If \"log2\", then max_features=log2(n_features).\n",
    "    4. If None, then max_features=n_features.\n",
    "- bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样\n",
    "- min_samples_split:节点划分最少样本数\n",
    "- min_samples_leaf:叶子节点的最小样本数\n",
    "- 超参数：n_estimator, max_depth, min_samples_split,min_samples_leaf\n",
    "\n",
    "   \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归\n",
    "    \n",
    "    目的：为了得到权重和偏置，进行预测\n",
    "\n",
    "    线性回归(Linear regression)是利用回归方程(函数)对一个或多个自变量(特征值)和因变量(目标值)之间【关系】进行【建模】的一种分析方式。【连续型】\n",
    "    分类是根据某种原理（信息熵，朴素贝叶斯等），实现分类\n",
    "    \n",
    "    线性回归 = 线性关系 + 非线性关系（高次项回归）\n",
    "     \n",
    "    特点：只有一个自变量的情况称为单变量回归，大于一个自变量情况的叫做多元回归\n",
    "    \n",
    "- 损失函数（最小二乘法）：误差的平方和\n",
    "- **基于均方误差最小化来进行模型求解的方法称为“最小二乘法”。**\n",
    "- 优化算法：找到最小损失对应的w值\n",
    "    1. 正规方程：\n",
    "    2. 梯度下降：\n",
    "    \n",
    " \n",
    "### 正规方程API：\n",
    "\n",
    "sklearn.linear_model.LinearRegression(fit_intercept=True)\n",
    "- fit_intercept：是否计算偏置\n",
    "- LinearRegression.coef_：回归系数\n",
    "- LinearRegression.intercept_：偏置\n",
    "\n",
    "### 梯度下降API：\n",
    "SGDRegressor类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型。\n",
    "sklearn.linear_model.SGDRegressor(loss=\"squared_loss\", fit_intercept=True, learning_rate ='invscaling', eta0=0.01)\n",
    "- loss:损失类型，loss=”squared_loss”: 普通最小二乘法\n",
    "- fit_intercept：是否计算偏置\n",
    "- learning_rate : 学习率填充（string, optional）\n",
    "    1. 'constant': eta = eta0\n",
    "    2. 'optimal': eta = 1.0 / (alpha * (t + t0)) [default]\n",
    "    3. 'invscaling': eta = eta0 / pow(t, power_t)，power_t=0.25:存在父类当中\n",
    "    4. 对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。\n",
    "- SGDRegressor.coef_：回归系数，权重\n",
    "- SGDRegressor.intercept_：偏置\n",
    "\n",
    "\n",
    "## 算法评估：\n",
    "\n",
    "均方误差回归损失API：\n",
    "    - from sklearn.metrics import mean_squared_error\n",
    "    - sklearn.metrics.mean_squared_error(y_true, y_pred)\n",
    "        - y_true:真实值\n",
    "        - y_pred:预测值\n",
    "        - return:浮点数结果\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欠拟合：学的太少，学不到\n",
    "    因为机器学习到的天鹅特征太少了，导致区分标准太粗糙，不能准确识别出天鹅。\n",
    "    \n",
    "### 过拟合：学太过了，包括噪声\n",
    "    机器已经基本能区别天鹅和其他动物了。然后，很不巧已有的天鹅图片全是白天鹅\n",
    "    \n",
    "训练所有条：过拟合\n",
    "训练m条：sgd，sag：防止过拟合\n",
    "训练一条数据：正确率不高\n",
    "\n",
    "### 正则化：\n",
    "    尽量减小高次项特征的影响：\n",
    "    正则化力度越大，权重系数越小\n",
    "    正则化力度越小，权重系数越大\n",
    "    L2： w趋近0，ridge回归 = 线性回归 + L2正则化\n",
    "    L1： w等于0，Lasso回归 = 线性回归 + L1正则化\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 岭回归（ridge回归）\n",
    "\n",
    "具有l2正则化的线性回归\n",
    "\n",
    "- API\n",
    "sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver=\"auto\", normalize=False)\n",
    "    - alpha:正则化力度，也叫 λ；正则化力度越大，权重系数越小；λ取值：0~1 1~10\n",
    "    - solver:会根据数据自动选择优化方法，可选正规方程，或SAG等优化方法\n",
    "        1. sag:如果数据集、特征都比较大，选择该随机梯度下降优化\n",
    "        2. normalize:数据是否进行标准化\n",
    "    - normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据\n",
    "Ridge.coef_:回归权重\n",
    "Ridge.intercept_:回归偏置\n",
    "\n",
    "\n",
    "- 具有l2正则化的线性回归，可以进行交叉验证API：\n",
    "sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)\n",
    "coef_:回归系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _BaseRidgeCV(LinearModel):\n",
    "    def __init__(self, alphas=(0.1, 1.0, 10.0),\n",
    "                 fit_intercept=True, normalize=False, scoring=None,\n",
    "                 cv=None, gcv_mode=None,\n",
    "                 store_cv_values=False):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归与二分类\n",
    "    \n",
    "逻辑回归（分类）= 线性回归（输出连续型）+ sigmoid函数+正则化（防止逻辑回归的过拟合）\n",
    "    \n",
    "- 输出结果：[0, 1]区间中的一个概率值\n",
    "    \n",
    "- 逻辑回归最终**的分类是通过属于某个类别的概率值来判断是否属于某个类别**\n",
    "\n",
    "- 正例（1），反例（0）\n",
    "    \n",
    "    - 默认将样本少的那一类当做1类，多的那一类当做0类\n",
    "    - 计算出来的是属于样本少那一类(正例)的概率值\n",
    "- 线性回归损失：均方误差\n",
    "- 逻辑回归损失：对数似然损失\n",
    "- 效果：提高训练预测的时候，真实值为1，对应的预测概率。减小目标值对应为0预测的概率\n",
    "- 优化：\n",
    "    - 梯度下降，提升原本属于1类别的概率，降低原本是0类别的概率。\n",
    "    - 不断的调整（1/0类别）概率，从而改变权重和偏置，使得损失最小\n",
    "    \n",
    "- API：\n",
    "sklearn.linear_model.LogisticRegression(solver='liblinear', penalty=‘l2’, C = 1.0)\n",
    "    - solver:优化求解方式（默认开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数）\n",
    "        - sag：根据数据集自动选择，随机平均梯度下降\n",
    "    - penalty：正则化的种类\n",
    "    - C：正则化力度\n",
    "    - lr = LogisticRegression()\n",
    "    \n",
    "默认将类别数量少的当做正例\n",
    "\n",
    "\n",
    "## 分类的评估方法\n",
    "    \n",
    "    线性回归评估方式：均方误差\n",
    "    分类算法评估方式：准确率，精确率与召回率\n",
    "    \n",
    "### 混淆矩阵\n",
    "    \n",
    "    在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)\n",
    "    \n",
    "    混淆矩阵 为了理解精确率与召回率\n",
    "- 精确率：预测结果为正例样本中真实为正例的比例（一般不看）\n",
    "    \n",
    "- 召回率：真实为正例的样本中预测结果为正例的比例（查的全，对正样本的区分能力）\n",
    "\n",
    "\n",
    "### 分类评估报告API\n",
    "   sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None )\n",
    "    - y_true：真实目标值\n",
    "    - y_pred：估计器预测目标值\n",
    "    - labels:指定类别对应的数字\n",
    "    - target_names：目标类别名称\n",
    "    - return：每个类别精确率与召回率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"精确率和召回率为：\", classification_report(y_test, lr.predict(x_test), labels=[2, 4], target_names=['良性', '恶性']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 衡量样本不均衡下的评估：\n",
    "    \n",
    "    此时不能用召回率衡量\n",
    "    如果准确率和召回率非常高，但是AUC非常低，说明此分类器不好\n",
    "    AUC(Area under Curve，Roc曲线下面积）\n",
    "    \n",
    "- ROC曲线与AUC指标\n",
    "    - ROC曲线\n",
    "    ROC曲线的横轴就是FPRate，纵轴就是TPRate，当二者相等时，表示的意义则是：对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的，此时AUC为0.5\n",
    "    - AUC指标：\n",
    "    AUC的概率意义是随机取一对正负样本，正样本得分大于负样本的概率\n",
    "    AUC的范围在[0.5, 1]之间，并且越接近1越好\n",
    "    AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。\n",
    "    0.5<AUC<1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。\n",
    "    AUC=0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。\n",
    "    AUC<0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在 AUC<0.5 的情况。\n",
    "\n",
    "- AUC指标为 大于0.7都是一些比较好的分类器\n",
    "- 样本不均衡，auc指标越重要\n",
    "\n",
    "    \n",
    "### AUC计算API：\n",
    "计算ROC曲线面积，即AUC值\n",
    "from sklearn.metrics import roc_auc_score\n",
    "- sklearn.metrics.roc_auc_score(y_true, y_score)\n",
    "- y_true:每个样本的真实类别，必须为【0(反例),1(正例)】标记\n",
    "- y_score:每个样本预测的概率值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型保存和加载\n",
    "\n",
    "## sklearn模型的保存和加载API\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "- 保存：joblib.dump(实例化估计器, 'test.pkl')\n",
    "- 加载：estimator = joblib.load('test.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无监督学习-K-means算法\n",
    "\n",
    "- 聚类一般做在分类之前\n",
    "\n",
    "- 根据样本数据特征的特点，给样本做分析。之所以称之为无监督，是因为归纳和分组是从无标签的数据开始学习的。\n",
    "\n",
    "## K-means 聚类 原理\n",
    "    \n",
    "    步骤：\n",
    "    1、随机设置K个特征空间内的点作为初始的聚类中心\n",
    "    2、对于其他每个点计算到K个中心点的距离(欧氏距离，k近邻算法)，非中心点选择最近的一个聚类中心点作为标记类别\n",
    "    3、接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）\n",
    "    4、如果计算得出的新中心点与原中心点一样，那么结束，否则重新进行第二步过程\n",
    "    \n",
    "    注：K值是分组的数量（已知）\n",
    "    \n",
    "    \n",
    "    1. 随机选出3个中心点（族群中心）\n",
    "    2. 计算其他的所有点，与3个中心点（族群中心）的距离，取三个值中最小（距离最近）的值对应的中心点，当做点的标记（分类），形成3个群体\n",
    "    3. 对三个群体，求出他们的平均值作为新的中心点。只要与原来的三个九中心店不一样，把新的中心点作为族群中心，重复第二步。若三个同时一样，聚类停止\n",
    "\n",
    "\n",
    "## K-meansAPI\n",
    "    sklearn.cluster.KMeans(n_clusters=8,init=‘k-means++’)\n",
    "\n",
    "        n_clusters:开始的聚类中心数量\n",
    "        init:初始化方法，默认为'k-means ++’\n",
    "        labels_:默认标记的类型，可以和真实值比较（不是值比较）\n",
    "        \n",
    "        \n",
    "## Kmeans性能评估指标\n",
    "    \n",
    "    外部距离(b_i)最大化，内部距离(a_i)最小化\n",
    "    \n",
    "### 轮廓系数\n",
    "\n",
    "    SC_i = (b_i - a_i) / max(b_i, a_i)\n",
    "    \n",
    "    注：对于每个点i 为已聚类数据中的样本 ，b_i 为i 到其它族群的所有样本的距离最小值，a_i 为i 到本身簇的距离平均值。最终计算出所有的样本点的轮廓系数平均值\n",
    "\n",
    "1. 选定一个蓝色的点 到族群内的其他点的距离平均值，当做a_i\n",
    "2. 这个蓝色的点 到其他两个族群:\n",
    "    - 到【红色族群】的【每个点】的【距离平均值】：红_i，\n",
    "    - 到【绿色族群】的【每个点】的【距离平均值】：绿_i, \n",
    "    - 取两者的最小值，当做b_i\n",
    "    \n",
    "- 取值范围：\n",
    "    取值范围：[-1, 1],越接近与1，聚类效果越好\n",
    "    聚类效果最好：b_i >> a_i, SC_i = 1 \n",
    "    聚类效果最坏：b_i << a_i, SC_i = -1\n",
    "    \n",
    "### 轮廓系数API\n",
    "计算所有样本的平均轮廓系数：\n",
    "sklearn.metrics.silhouette_score(X, labels)\n",
    "    - X：特征值\n",
    "    - labels：被聚类标记的目标值\n",
    "用户聚类结果评估    \n",
    "    silhouette_score(cust, predict)\n",
    "    \n",
    "### K-means总结\n",
    "    特点分析：采用迭代式算法，直观易懂并且非常实用\n",
    "    缺点：容易收敛到局部最优解(多次聚类)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习和深度学习的区别\n",
    "\n",
    "  * 特征提取方面\n",
    "    * 机器学习：手动进行特征工程\n",
    "    * 深度学习：算法自动筛选提取\n",
    "      * 适合用在难提取特征的图像、语音、自然语言领域\n",
    "  * 数据量的大小\n",
    "    * 机器学习：数据量偏小\n",
    "    * 深度学习：数据量非常大\n",
    "      * 导致计算时间比较长，需要各种计算设备\n",
    "  * 算法上的区别\n",
    "    * 机器学习：朴素贝叶斯、决策树等\n",
    "    * 深度学习：神经网络\n",
    "    \n",
    "深度学习应用场景：\n",
    "  * 图像、语音、文本\n",
    "  * 其它都可以去进行尝试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习之TensorFlow\n",
    "\n",
    "TensorFlow是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。\n",
    "- 节点（Operation）在图中表示数学操作，\n",
    "- 图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor）。\n",
    "\n",
    "\n",
    "## Tensorflow程序当中的重要组成部分(结构)\n",
    "\n",
    "两部分：\n",
    "\n",
    "  * **一个是构建图阶段**：图（程序）的定义\n",
    "    * 张量：TensorFlow 中的基本数据对象\n",
    "    * 节点(OP): 指的也是运算操作（也包括也一些不仅是提供）\n",
    "  * **一个是执行图阶段**：会话去运行图程序\n",
    "\n",
    "### TensorFlow围绕图，会话，张量，节点(OP) 展开 \n",
    "图和会话 ：\n",
    "    - 图：这是 TensorFlow 将计算表示为指令之间的依赖关系的一种表示法\n",
    "    - 会话：TensorFlow 跨一个或多个本地或远程设备运行数据流图的机制\n",
    "    - 张量：TensorFlow 中的基本数据对象\n",
    "    - 节点：提供图当中执行的操作\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. 图与Tensorboard\n",
    "\n",
    "  * 图包含了一组**tf.Operation代表计算单元（节点OP）的对**象和**tf.Tensor（张量）代表计算单元之间流动的数据**\n",
    "\n",
    "  * Tensorboard: 为了更方便 TensorFlow 程序的理解、调试与优化\n",
    "  \n",
    "使用可视化学习工具的步骤：\n",
    "1. 在会话中，将程序数据序列化到events文件：\n",
    "   - 方法：tf.summary.FileWriter(\"./\", graph=sess.graph)\n",
    "   - 文件名：events.out.tfevents.{timestamp}.{hostname}\n",
    "2. 启动TensorBoard\n",
    "   - 终端输入：tensorboard --logdir=\"./\"\n",
    "   - 网页搜索栏输入：http://localhost:6006/\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "## 2. OP（节点）\n",
    "\n",
    "只要是tf下的API我们都可以称之为一个OP，也称之为指令\n",
    "\n",
    "- OP有一个名字：在整个tensorflow程序当中全局唯一\n",
    "    - Tensor(\"Const:0\", shape=(), dtype=float32)\n",
    "    - Tensor(\"Const_1:0\", shape=(), dtype=float32)\n",
    "- 通过name参数，修改指令名称，便于查看图当中的OP内容\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 3. 会话（运行程序并分配运行资源）\n",
    "   \n",
    "一个运行TensorFlow operation的类。会话包含以下两种开启方式：\n",
    "- tf.Session：用于完整的程序当中\n",
    "- tf.InteractiveSession：用于交互式上下文中的TensorFlow ，例如shell\n",
    "\n",
    "* 需要close关闭会话、释放资源\n",
    "\n",
    "* 会话运行的是默认这张图，所以要运行的OP必须是这张图当中的\n",
    "  * **通过graph参数去修改运行图，当然修改了图就要运行对应图当中的OP**\n",
    "\n",
    "* run:参数：\n",
    "      * fetches:运行多个使用列表，运行对象不能是一些非op,tensor一些对象，比如int, double\n",
    "      * **feed_dict:运行时候提供数据，一般不确定数据形状时，可以结合placeholder去使用**。用在训练的时候实时提供要训练的批次数据\n",
    "      \n",
    "报错：\n",
    "- RuntimeError：如果这Session是无效状态（例如已关闭）。\n",
    "- TypeError：如果fetches或者feed_dict键的类型不合适。\n",
    "- ValueError：如果fetches或feed_dict键无效或引用 Tensor不存在的键。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 4. 张量\n",
    "\n",
    "TensorFlow 的张量就是一个 n 维数组， 类型为tf.Tensor。Tensor具有以下两个重要的属性：\n",
    "\n",
    "- type:数据类型\n",
    "- shape:形状(阶)\n",
    "\n",
    "【类型转换】使用tf.cast()\n",
    "【形状转换】关于动态形状和静态形状必须符合以下规则:\n",
    "\n",
    "1.静态形状:\n",
    "- 转换静态形状的时候，1阶到1阶，2阶到2阶，【不能跨阶数改变形状】\n",
    "- 对于已经固定的张量的静态形状的张量，不能再次设置静态形状；如果形状不固定，可以修改张量的形状\n",
    "- 修改的是本身的形状\n",
    "- tf.set_shape:修改的张量本身的形状、固定的形状不能修改，也不能跨阶数修\n",
    "\n",
    "2.动态形状:\n",
    "- tf.reshape:会创建新的张量，不修改原来张量的形状，注意元素数量匹配       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用close()方法\n",
    "sess = tf.Session()\n",
    "sess.run(...)\n",
    "sess.close()\n",
    "\n",
    "# 使用上下文管理器with\n",
    "with tf.Session() as sess:\n",
    "  sess.run(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变量OP：特殊的创建张量值的OP指令\n",
    "  * **1、显示初始化：sess.run(tf.global_variables_initializer())**\n",
    "  * 2、可以通过assign修改原来的变量op当中张量的值, assign_add也是可以去修改原来的值，加上一个新的值\n",
    "  * 3、命名空间与共享变量（全局变量）name全局唯一\n",
    "    * tfvariable_scope去创建，会在op的名字前面继续加上空间的名字，与共享变量结合使用\n",
    "    * tf.get_variable：如果一个命名空间当中存在名字一样的，那么会报冲突错误，怎么解决？\n",
    "      * with tf.variable_scope(\"my_scope1\") as scope+ scope.reuse_variables()\n",
    "      * with tf.variable_scope(\"name\", reuse=tf.AUTO_REUSE):\n",
    "      * var2 = tf.get_variable**(initializer=tf.random_normal([2, 3]**, mean=0.0, stddev=1.0),\n",
    "                                   name=\"var2\")\n",
    "    * **Tensorflow:维护一个所以OP名字的列表，不是以取的名字（自定义接受结果，python变量）去区分**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归案例\n",
    "\n",
    "步骤\n",
    "1. 准备数据x, y_true\n",
    "2. 根据数据建立线性模型：\n",
    "    - w，b: 随机初始化一个权重和偏置\n",
    "    - y_predict = x*w + b\n",
    "    - x * w是矩阵运算： **tf.matmul(x, w)**\n",
    "3. 利用y_predict与y_true求出均方误差，建立损失 loss\n",
    "4. 利用梯度下降减少模型误差，从而优化模型参数，sgd_op\n",
    "    - 学习率指定[0,1]\n",
    "    \n",
    "### 运算API：\n",
    "\n",
    "矩阵运算\n",
    "- tf.matmul(x, w)\n",
    "平方\n",
    "- tf.square(error)\n",
    "均值\n",
    "- tf.reduce_mean(error)\n",
    "\n",
    "对每个样本的损失求和，然后再求平均值(相减，平方，相加求平均值)\n",
    "- loss = tf.reduce_mean(tf.square(y_true - y_predict))\n",
    "\n",
    "梯度下降优化\n",
    "\n",
    "\n",
    "\n",
    "tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "learning_rate:学习率，一般为0~1之间比较小的值\n",
    "return:梯度下降op\n",
    "\n",
    "\n",
    "* 实现线性回归\n",
    "  * 1、学习率、步长以及梯度爆炸\n",
    "    * 学习率不应该设置过大、0~1之间的数，如果过大，导致梯度爆炸（损失、参数优化成nan）\n",
    "    * 学习率越大， 达到最终比较好的效果步长越小\n",
    "    * 学习率越小， 达到最终比较好的效果步长越大\n",
    "    * 大家都会选择比较小的学习率：**0.01,0.001,0.0001,0.00001**\n",
    "      * 梯度爆炸解决方案：1、重新设计网络\n",
    "        **2、调整学习率**\n",
    "        3、使用梯度截断（在训练过程中检查和限制梯度的大小）\n",
    "        4、使用激活函数\n",
    "  * 2、模型的要优化参数必须选择tf.Variable定义，并且可以指定trainable参数指定是否被训练\n",
    "  * 3、模型的保存和加载\n",
    "    * 文件格式：checkpoint文件\n",
    "    * 保存：指定路径，**指定要保存的会话**，默认保存tf.Variable的OP，可以指定保存哪些\n",
    "    * 加载：本地文件的模型当中的一些变量名字与要进行加载训练的代码中的变量名字必须一致，代码训练以本地文件模型的参数值继续训练\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度紊乱：学习率过大\n",
    "梯度爆炸：\n",
    "梯度爆炸就是由于初始化权值过大，前面层会比后面层变化的更快，就会导致权值越来越大，梯度爆炸的现象就发生了。\n",
    "\n",
    "梯度消失：权重越来越小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文件读取流程\n",
    "\n",
    "\n",
    "1. 构造一个【路径+文件名】的队列\n",
    "2. 进行文件名队列的读取，并进行解码操作\n",
    "3. 放到样本当中，进行批处理\n",
    "\n",
    "文件种类：\n",
    "1. CSV：默认一次只读一行\n",
    "2. 图片文件：默认只读一张\n",
    "3. 二进制文件：指定一个样本的bytes读取，一次只读一个样本\n",
    "\n",
    "## API：\n",
    "\n",
    "### 第一阶段：构造文件队列，将需要读取的文件装入到一个固定的队列当中\n",
    "tf.train.string_input_producer(string_tensor,shuffle=True)\n",
    "- string_tensor：含有文件名+路径的1阶张量\n",
    "- num_epochs:过几遍数据，默认无限过数据\n",
    "- return 文件队列\n",
    "\n",
    "\n",
    "### 第二阶段：从队列当中（有规则的）读取文件内容，并且进行解码操作。\n",
    "\n",
    "\n",
    "#### 1. 读取文件内容：\n",
    "TensorFlow默认每次只读取一个样本，具体到\n",
    "- 文本文件读取一行、\n",
    "- 二进制文件读取指定字节数(最好一个样本)、\n",
    "- 图片文件默认读取一张图片、\n",
    "- TFRecords默认读取一个example\n",
    "\n",
    "实例化一个阅读器：\n",
    "- tf.TextLineReader:阅读文本文件逗号分隔值（CSV）格式,默认按行读取，返回读取器实例\n",
    "- tf.WholeFileReader:用于读取图片文件\n",
    "- tf.TFRecordReader:读取TFRecords文件\n",
    "- tf.FixedLengthRecordReader（all_bytes）:二进制文件\n",
    "    - 要读取每个记录是固定数量字节的二进制文\n",
    "    - record_bytes:整型，指定每次读取(一个样本)的字节数\n",
    "    - return：读取器实例\n",
    "\n",
    "1、他们有共同的读取方法：read(file_queue)：从队列中指定数量内容返回一个Tensors元组（key文件名字，value默认的内容(一个样本)）\n",
    "**key, value = 读取器实例.read(file_queue)**\n",
    "\n",
    "2、由于默认只会读取一个样本，所以通常想要进行批处理。使用tf.train.batch或tf.train.shuffle_batch进行多样本获取，便于训练时候指定每批次多个样本的训练\n",
    "\n",
    "#### 2. 内容解码：\n",
    "对于读取不通的文件类型，内容需要解码操作，解码成统一的Tensor格式\n",
    "\n",
    "- tf.decode_csv：解码文本文件内容\n",
    "- tf.decode_raw(value,tf.uint8)：解码二进制文件内容\n",
    "    - 与tf.FixedLengthRecordReader搭配使用，二进制读取为uint8格式\n",
    "- tf.image.decode_jpeg(value)\n",
    "    - 将JPEG编码的图像解码为uint8张量\n",
    "    - return:uint8张量，3-D形状[height, width, channels]\n",
    "- tf.image.decode_png(value)\n",
    "    - 将PNG编码的图像解码为uint8张量\n",
    "    - return:张量类型，3-D形状[height, width, channels]\n",
    "解码阶段，默认所有的内容都解码成tf.uint8格式，如果需要后续的类型处理继续处理\n",
    "\n",
    "### 要固定tensor的形状，不能带有“？”\n",
    "\n",
    "### 第三阶段: 批处理\n",
    "\n",
    "在解码之后，我们可以直接获取默认的一个样本内容了，但是如果想要获取多个样本，这个时候需要结合管道的末尾进行批处理\n",
    "\n",
    "读取指定大小（个数）的张量：\n",
    "tf.train.batch(tensors,batch_size,num_threads = 1,capacity = 32,name=None)\n",
    "- tensors：可以是【包含张量的列表】,批处理的内容放到列表当中\n",
    "- batch_size:从队列中读取的批处理大小\n",
    "- num_threads：进入队列的线程数\n",
    "- capacity：整数，队列中元素的最大数量（队列大小）\n",
    "- 一般batch_size=capacity\n",
    "- return:tensors\n",
    "tf.train.shuffle_batch\n",
    "\n",
    "\n",
    "## 线程操作：\n",
    "\n",
    "- 以上的创建这些队列和排队操作称之为tf.train.QueueRunner\n",
    "- 每个QueueRunner都负责一个阶段，并拥有需要在线程中运行的排队操作列表。\n",
    "- 一旦图形被构建， tf.train.start_queue_runners 函数就会要求图中的每个QueueRunner启动它的运行排队操作的线程\n",
    "- 总结：\n",
    "    - 文件读取是【线程】和【队列】合作的流程，\n",
    "    - 主线程执行main里面的内容，但由于没有收到数据，故阻塞，\n",
    "    - 执行函数的是子线程，需要手动开启线程，读取数据，填充到批处理，并返回给主线程（进行操作）。\n",
    "\n",
    "API：\n",
    "1. 实例化Coord线程协调器：\n",
    "    - tf.train.Coordinator()\n",
    "        - 线程协调员,实现一个简单的机制来协调一组线程的终止\n",
    "        - request_stop()：请求停止\n",
    "        - should_stop()：询问是否结束\n",
    "        - join(threads=None,stop_grace_period_secs=120)：回收线程\n",
    "        - return:线程协调员实例\n",
    "\n",
    "2. 收集所有图中的队列线程，并启动线程：\n",
    "    - tf.train.start_queue_runners(sess=None,coord=None)\n",
    "\n",
    "        - sess:所在的会话中\n",
    "        - coord：线程协调器\n",
    "        - return：返回所有线程\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 图片特征值预处理\n",
    "\n",
    "在进行图片识别的时候，每个图片样本的特征数量要保持相同，需要缩小放大图片成同一尺寸。\n",
    "\n",
    "- tf.image.resize_images(images, size)\n",
    "    - images：4-D形状[batch, height, width, channels]或3-D形状的张量[height, width, channels]的图片数据\n",
    "    - size：1-D int32张量：new_height, new_width，图像的新尺寸\n",
    "    \n",
    "### 数据类型：运算的时候：需要float32类型     读取出来解码的类型：uint8格式\n",
    "\n",
    "### tf.tanspose(tensor, [1,2,0])\n",
    "1，2，0 对应的一维二维三维\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要手动开启线程协调员:\n",
    "    \n",
    "# 创建线程回收的协调员\n",
    "    coord = tf.train.Coordinator()\n",
    "\n",
    "    # 需要手动开启子线程去进行批处理读取到队列操作\n",
    "    threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "\n",
    "    print(sess.run(image_batch))\n",
    "\n",
    "    # 回收线程\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "假设1, 2, 3, 4-红色； 5, 6, 7, 8-绿色； 9, 10, 11, 12-蓝色\n",
    "如果通道在最低维度0[channel, height, width]，RGB三颜色分成三组，在第一维度上找到三个RGB颜色\n",
    "如果通道在最高维度2[height, width, channel]，在第三维度上找到RGB三个颜色\n",
    "\n",
    "\n",
    "# 1、想要变成：[2 height, 2width,  3channel]，但是第三维(列)的输出结果不对(应该是每列：1 2 3 4，5 6 7 8，9 10 11 12)\n",
    "In [7]: tf.reshape([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], [2, 2, 3]).eval()\n",
    "Out[7]:\n",
    "array([[[ 1,  2,  3],\n",
    "        [ 4,  5,  6]],\n",
    "\n",
    "       [[ 7,  8,  9],\n",
    "        [10, 11, 12]]], dtype=int32)\n",
    "\n",
    "# 2、所以要这样去做\n",
    "In [8]: tf.reshape([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], [3, 2, 2]).eval()\n",
    "Out[8]:\n",
    "array([[[ 1,  2],\n",
    "        [ 3,  4]],\n",
    "\n",
    "       [[ 5,  6],\n",
    "        [ 7,  8]],\n",
    "\n",
    "       [[ 9, 10],\n",
    "        [11, 12]]], dtype=int32)\n",
    "# 接着使用tf.transpose ，0，1，2代表三个维度标记\n",
    "# Convert from [depth, height, width] to [height, width, depth].\n",
    "# 0,1,2-----> 1, 2, 0\n",
    "In [17]: tf.transpose(depth_major, [1, 2, 0]).eval()\n",
    "Out[17]:\n",
    "array([[[ 1,  5,  9],\n",
    "        [ 2,  6, 10]],\n",
    "\n",
    "       [[ 3,  7, 11],\n",
    "        [ 4,  8, 12]]], dtype=int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecords文件\n",
    "1. 特点：\n",
    "    更好的利用内存，更方便复制和移动，并且**不需要单独的标签文件**。\n",
    "\n",
    "2. 作用：\n",
    "    TFRecords文件包含了tf.train.Example 协议内存块(协议内存块包含了字段 Features)。\n",
    "    可以获取你的数据， 将数据填入到Example协议内存块，将协议内存块序列化为一个字符串， 并且通过tf.python_io.TFRecordWriter 写入到TFRecords文件。\n",
    "    通过example模块存储的特征值/目标值，不需要再次切分，保存什么取出什么。反例：二进制也可以把标签和特征值存在一起，但读取后，又要分割，还要处理类型\n",
    "\n",
    "\n",
    "3. example模块：\n",
    "    tf.train.Example 协议内存块(protocol buffer)(协议内存块包含了字段 Features)，Features包含了一个Feature字段，Features中包含要写入的数据、并指明数据类型。\n",
    "    \n",
    "好处：\n",
    "    方便存储更多的图片的信息，特征值、目标值、通道等等，不需要更多去处理读取出来的结果\n",
    "\n",
    "4. 流程：\n",
    "    构造example协议，在指定好数据类型，最后序列化写入example.SerializeToString()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                \"image\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),\n",
    "                \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
    "            }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 存储：\n",
    "\n",
    "    * tf.python_io.TFRecordWriter(\"./tmp/cifar.tfrecords\")\n",
    "    * 循环将数据构造`Example`协议内存块(protocol buffer)序列化之后写入文件\n",
    "    * 构造example:tf.train.Int64List(value=[Value]),tf.train.BytesList(value=[Bytes]),tf.train.FloatList(value=[value]) ，byets需要序列\n",
    "    *  example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                      \"image\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),\n",
    "                      \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),\n",
    "                  }))\n",
    "* 读取\n",
    "    * 通文件读取流程\n",
    "    * 在读取内容之后，需要增加一个解析example协议的步骤\n",
    "    * 类型只能是float32,int64,string\n",
    "    * feature = tf.parse_single_example(values, features={\n",
    "                  \"image\": tf.FixedLenFeature([], tf.string),\n",
    "                  \"label\": tf.FixedLenFeature([], tf.int64)\n",
    "              })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN、NN深度学习--神经网络\n",
    "\n",
    "* **输入层，输出层以及隐藏层。**\n",
    "\n",
    "神经网络的特点:\n",
    "1. 每个连接都有个权值,\n",
    "2. 同一层神经元之间没有连接,\n",
    "3. 最后的输出结果对应的层也称之为全连接层\n",
    "\n",
    "## 感知机：\n",
    "\n",
    "- 单个感知机是以个神经元\n",
    "  * **这个感知机具有连接的权重和偏置还有一个sign(阶跃函数)**，输出0或者1\n",
    "  \n",
    "        u = ∑(wi*xi)+b\n",
    "        y=sign(u)= 1, u>0\n",
    "                   -1, u<0\n",
    "\n",
    "  * 神经网络：多个神经元（感知机）组成，然后解决分类问题\n",
    "\n",
    "\n",
    "# 神经网络原理：\n",
    "\n",
    "- 神经网络的主要用途在于分类，神经网络解决多分类问题最常用的方法是在全连接层(输出层)设置n个输出节点，其中n为类别的个数。\n",
    "    - 如果是十分类问题，则输出10个结果\n",
    "\n",
    "\n",
    "- 与问题：每个样本的两个特征同时为1，结果为1\n",
    "- 或问题：每个样本的两个特征一个为1，结果为1\n",
    "- 异或：每个样本的两个特征相同为0， 不同为1\n",
    "\n",
    "\n",
    "## softmax回归（解决分类问题）：\n",
    "\n",
    "- 衡量分成哪个类别\n",
    "\n",
    "- 作用于网络输出的最后一层，将神经网络**输出**转换成**概率**结果\n",
    "    - softmax(y_i) = e^(y_i) / ∑e^(y_j), (j=1,n)\n",
    "    \n",
    "- 每个分类的概率之和为1. \n",
    "    \n",
    "- 假设输出结果y1 y2 y3 为：2.3, 4.1, 5.6，softmax的计算输出结果为：\n",
    "    - y1_p = e^2.3/(e^2.3+e^4.1+e^5.6)\n",
    "    - y2_p = e^4.1/(e^2.3+e^4.1+e^5.6)\n",
    "    - y3_p = e^5.6/(e^2.3+e^4.1+e^5.6)\n",
    "     \n",
    "      \n",
    "## 交叉熵损失：\n",
    "\n",
    "- 提高对应目标值为1的位置输出概率大小\n",
    "\n",
    "（与信息熵相似）\n",
    "\n",
    "- 为了能够衡量误差，目标值需要进行one-hot编码，能与概率值一一对应\n",
    "\n",
    "公式：\n",
    "- Hy'(y) = -∑[y_i'log(y_i)]\n",
    "    - y_i':真实类别（0或1）\n",
    "    - y_i:预测概率值\n",
    "    \n",
    "- 例：0log(0.10)+0log(0.05)+0log(0.15)+0log(0.10)+0log(0.05)+0log(0.20)+1log(0.10)+0log(0.05)+0log(0.10)+0log(0.10)\n",
    "\n",
    "    - 为了减少损失-->减少1log(0.10)的值-->增大0.10，当log(1)=0时，损失最小。\n",
    "\n",
    "\n",
    "- 减少损失-->减少log(y_i)-->增大y_i，当log(1)=0时，损失最小。\n",
    "\n",
    "提高真实值对应的预测概率，同时由于softmax回归，其他概率值会减小(相加等于1)，已达到预测的效果\n",
    "\n",
    "    \n",
    "## 优化算法：BP算法（Backpropagation）\n",
    "\n",
    "\n",
    "- 反向传播算法：\n",
    "\n",
    "        神经网络当中充满大量的权重、偏置参数，这些参数都需要去进行优化。由于神经网络的隐层可以增加很多层，那么这个过程需要一种规则，即BP\n",
    "\n",
    "- 定义：**梯度下降+链式求导规则**\n",
    "\n",
    "\n",
    "    1. 前向传输（Feed-Forward）\n",
    "\n",
    "        - 从输入层=>隐藏层=>输出层，一层一层的**计算预测**所有神经元输出值的过程。\n",
    "\n",
    "    2. 逆向反馈（Back Propagation）\n",
    "\n",
    "        - 因为输出层的值与真实的值会存在误差，我们可以用**交叉熵损失**来衡量预测值和真实值之间的误差。\n",
    "        \n",
    "        - 在手工设定了神经网络的层数，每层的神经元的个数，学习率 η后，BP 算法会先**随机初始化每条连接线权重和偏置**\n",
    "        \n",
    "        - 对于训练集都会先执行**前向传输**得到预测值\n",
    "        \n",
    "        - 根据真实值与预测值之间的误差执行**逆向反馈**，更新神经网络中每条连接线的权重和每层的偏置。\n",
    "\n",
    "\n",
    "- 总结：\n",
    "    - 训练过程中的计算机会尝试一点点增大或减小每个参数，看其能如何减少相比于训练数据集的误差，以望能找到最优的权重、偏置参数组合\n",
    "\n",
    "\n",
    "## softmax, 交叉熵损失API：\n",
    "\n",
    "- 计算logits和labels之间的交叉损失熵：\n",
    "\n",
    "    - tf.nn.softmax_cross_entropy_with_logits(labels=None, logits=None,name=None)\n",
    "    \n",
    "        - labels:标签值（真实值**one hot 编码**）\n",
    "        - logits：样本加权之后的值（预测值）\n",
    "        - return:返回损失值列表\n",
    "        \n",
    "        - 先softmax计算概率，再计算损失\n",
    "\n",
    "- 计算出来的是每个样本的损失，还需要计算平均损失\n",
    "\n",
    "- 计算张量的尺寸的元素平均值（平均每个样本的损失）\n",
    "\n",
    "    tf.reduce_mean(input_tensor)\n",
    "\n",
    "- 再用梯度下降优化：\n",
    "    \n",
    "    - tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        - learning_rate:学习率，一般为0~1之间比较小的值\n",
    "        - return:梯度下降op\n",
    "        \n",
    "## 准确率：\n",
    "    \n",
    "- 每个样本的预测结果（概率）与真实结果（one-hot）的对比\n",
    "    - 若两者的最大值，一一对应，则将样本设置成1，否则为0\n",
    "    - 准确率 = 结果为1的样本的总数 / 总的样本数 = reduce_mean(结果为1的样本的总数)\n",
    "- API:\n",
    "    - equal_list = tf.equal(tf.argmax(y, 1), tf.argmax(y_label, 1))\n",
    "    - accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32))    # eq_li的计算结果是10等，所以要进行cast转换\n",
    " \n",
    " \n",
    "## 添加tensorboard变量观察\n",
    "\n",
    "- 建立时间文件\n",
    "- 收集张量值、合并张量值\n",
    "- 运行合并的OP\n",
    "- 写入文件（每批次的结果）\n",
    "\n",
    "\n",
    " \n",
    "## 流程：\n",
    "    \n",
    "- N个样本，每个样本400个特征，分成10组：\n",
    "    - 权重总数：400*10\n",
    "    - 特征值（N，400） * 权重（400，10） + b = 预测值（N，10）N个样本，每个样本10个输出\n",
    "    - 把预测值放入softmax计算出概率，用交叉熵计算出损失，用bp去优化损失\n",
    "\n",
    "\n",
    "\n",
    "## 线性神经网络的局限性\n",
    "\n",
    "### 加100个隐层都没用\n",
    "\n",
    "任意**多个隐层**的神经网络和**单层**的神经网络都**没有区别**，而且都是线性的，而且线性模型的能够解决的问题也是有限的\n",
    "    \n",
    "    解决办法：\n",
    "    1. 添加激活函数Relu\n",
    "    2. 改变神经网络结构\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# 卷积神经网络\n",
    "\n",
    "\n",
    "## 卷积神经网络与传统多层神经网络对比\n",
    "\n",
    "- 传统多层神经网络没有理论指出多少层（层数没有作用）\n",
    "\n",
    "\n",
    "- 卷积网络：更加有效的特征学习部分，**使得加深网络得以有效**\n",
    "\n",
    "\n",
    "- 深度学习：卷积神经网络、循环神经网络，**新的结构以及一些新的方法（比如新的激活函数Relu等）**\n",
    "\n",
    "\n",
    "## 卷积神经网络发展：\n",
    "\n",
    "- 网络结构加深\n",
    "\n",
    "- 加强卷积功能\n",
    "\n",
    "- 从分类到检测\n",
    "\n",
    "- 新增功能模块\n",
    "\n",
    "\n",
    "## 原理：\n",
    "\n",
    "### 1. 结构：卷积层+激活函数+池化层 = 隐藏层\n",
    "\n",
    "- 卷积层：通过在原始图像上平移来提取特征\n",
    "\n",
    "- 激活层：增加非线性分割能力\n",
    "\n",
    "- 池化层：通过特征后稀疏参数来减少学习的参数，降低网络的复杂度，（最大池化和平均池化）\n",
    "\n",
    "- （非卷积必须）输出分类时：为了能够达到分类效果，还会有一个全连接层(FC)也就是最后的输出层，进行损失计算分类。\n",
    "\n",
    "\n",
    "### 2. 卷积层：\n",
    "\n",
    "\n",
    "- 组成：卷积神经网络中每层卷积层由**若干卷积单元(卷积核)** 组成\n",
    "\n",
    "- 特点：**提取特征**的功能更加强大。无法解释每层卷积层具体做什么，只是形象的比喻\n",
    "\n",
    "#### 2.1 卷积核：\n",
    "\n",
    "- 作用：\n",
    "    - 每个卷积核(观察者)，带着若干权重和一个偏置，把整张图片观察完整，进行特征加权运算(线性回归运算)，得出一个结果\n",
    "\n",
    "- 每层卷积层中，不同卷积核的**步长**和**大小**都是一样的，但这些卷积核的权重和偏置**不一样**\n",
    "\n",
    "- 四要素：\n",
    "    - 卷积核个数\n",
    "    \n",
    "        - 每层卷积层中，**卷积核的数量**就是得出**结论**的数量\n",
    "   \n",
    "    - 卷积核大小\n",
    "    \n",
    "        - 观察窗口的大小\n",
    "        \n",
    "        - 经证明，卷积核大小通常为: 1x1、3x3、5x5\n",
    "        \n",
    "        - 每个卷积核带有若干个**权重**和一个**偏置**\n",
    "        \n",
    "        - 特征加权运算\n",
    "        \n",
    "    - 卷积核步长\n",
    "    \n",
    "        - 为了把整张图片观察完整，需要移动观察窗口\n",
    "        \n",
    "        - 每次观察，得出一个结果\n",
    "        \n",
    "        - 通常采用一个像素步长（步长越大，观察运算越粗糙）\n",
    "        \n",
    "        - 一个卷积核的移动，得出一个观察后的结果\n",
    "        \n",
    "    - 卷积核零填充大小\n",
    "    \n",
    "        - 每个人得出结论的大小[2,2]，不仅由卷积核大小和步长决定，还由零填充大小决定\n",
    "        \n",
    "        - 采取超过图像大小去观察，不损失图片的像素信息\n",
    "            - 移动的步长，不能被图片的总像素整除\n",
    "            \n",
    "        - 为什么是0值\n",
    "            - 因为特征加权运算，0值不会影响像素运算结果\n",
    "            \n",
    "        - 有两种方式，SAME和VALID\n",
    "           \n",
    "        \n",
    "- 每个卷积核(观察者)得出的结论大小，跟卷积核的大小、步长和零填充有关\n",
    "\n",
    "    - 假设5个人去观察，每个人得出的结果是2*2\n",
    "    - 得出5个2X2的结果，[5, 2, 2]\n",
    "    \n",
    "- 先知道卷积核大小，移动步长，零填充大小，才能计算最总结果的大小\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2 输出大小计算公式\n",
    "\n",
    "最终零填充到底填充多少，并不需要去关注。可以利用已知的这些条件求出输出的大小来看结果\n",
    "\n",
    "- 想要什么样的结果，就可以倒推计算零填充的大小\n",
    "\n",
    "- 输入体积大小：\n",
    "    - H1 x W1 x D1\n",
    "\n",
    "- 四个超参数：\n",
    "    - Filter数量：k\n",
    "    - Filter大小：f\n",
    "    - 步长：s\n",
    "    - 零填充大小：p\n",
    "\n",
    "- 输出体积大小：（黑白，彩色图片都适用）\n",
    "    - H2 = (H1 - f(filter_Height) + 2p) / s + 1\n",
    "    - W2 = (W1 - f(filter_width) + 2p) / s + 1\n",
    "    - D2 = k\n",
    "    \n",
    "    \n",
    "- 例子：\n",
    "    1. 假设已知的条件：输入图像32*32*1, 50个Filter，大小为5*5，移动步长为1，零填充大小为1。请求出输出大小？\n",
    "\n",
    "        - H2 = (H1 - f + 2p) / s + 1 = (32 - 5 + 2 * 1)/1 + 1 = 30\n",
    "        - W2 = (W1 - f + 2p) / s + 1 = (32 -5 + 2 * 1)/1 + 1 = 30\n",
    "        - D2 = K = 50\n",
    "\n",
    "        - 所以输出大小为[30, 30, 50]\n",
    "\n",
    "    2. 假设已知的条件：输入图像32*32*1, 50个Filter，大小为3*3，移动步长为1，未知零填充。输出大小32*32？\n",
    "\n",
    "        - H2 = (H1 - f + 2p) / s + 1 = (32 - 3 + 2 * P)/1 + 1 = 32\n",
    "        - W2 = (W1 - f + 2p) / s + 1 = (32 -3 + 2 * P)/1 + 1 = 32\n",
    "        \n",
    "        - 所以零填充大小为：1*1\n",
    "          \n",
    "- 多通道图片的观察：\n",
    "    - 对于彩色图片：每个人去观察的时候，需要3张权重表，最后还是得出一张表结论\n",
    "\n",
    "\n",
    "- 如果是一张彩色图片，那么就有三种表分别为R，G，B。**原本每个人需要带一个3x3或者其他大小的卷积核，现在需要带3张3x3的权重和一个偏置，总共就27个权重。**最终每个人还是得出一张结果\n",
    "\n",
    "\n",
    "- 结论: 结果大小，根据卷积运算公式去计算。不同的卷积核观察出不同的结果（图片的特征）\n",
    "\n",
    "\n",
    "\n",
    "#### 2.3 卷积核API：\n",
    "\n",
    "计算给定4-D input和filter张量的2维卷积\n",
    "\n",
    "- tf.nn.conv2d(input, filter, strides=, padding=, name=None)\n",
    "\n",
    "    - input：给定的输入**4维**张量，具有[batch,heigth,width,channel]，类型为float32,64，batch：指定观察多少张图片\n",
    "    \n",
    "    - filter：指定过滤器的权重数量，[filter_height, filter_width, in_channels, out_channels]。\n",
    "        \n",
    "        - filter_height, filter_width： 卷积核的大小\n",
    "        \n",
    "        - in_channels：输入的图片通道数。如果是黑白通道，则是1；如果是彩色，则是3\n",
    "        \n",
    "        - out_channels：输出的通道，也就是**卷积核的数量**\n",
    "        \n",
    "            - out_channels个人，每个人带in_channels（3）张图片，每张图片大小为(fh, fw)\n",
    "        \n",
    "        - strides：strides = [1, stride, stride, 1],步长\n",
    "            - 第一个stride 左右移动步长\n",
    "            - 第二个stride 上下移动步长\n",
    "            - 两个1是为了与其他参数**维度匹配**\n",
    "            \n",
    "        - padding：“SAME”, “VALID”，使用的填充算法的类型：\n",
    "        \n",
    "            - SAME（常用）：越过边缘取样(零填充后)，**输出高宽与输入大小一样（重要）**\n",
    "                - 输入28*28，则输出28*28\n",
    "                - 自动计算并填写0填充大小\n",
    "                \n",
    "            - VALID：不越过边缘取样，取样的面积小于输入的图像的像素宽度。\n",
    "\n",
    "\n",
    "### 3. 激活函数\n",
    "\n",
    "- 增加非线性函数的分割能力\n",
    "\n",
    "- 如果观察的结果小于0，则把结果都设置成0；大于0的结果，等于结果本身\n",
    "\n",
    "- ReLU = max(0,x)  \n",
    "    - 图片没有负值\n",
    "    - 有效解决梯度爆炸问题、计算速度非常快（公式）\n",
    "- sigmoid缺点: 1/(1+e^-(y_predict))\n",
    "      采用sigmoid等函数，计算量相对大，sigmoid函数**反向传播**时，很容易就会出现梯度梯度爆炸现象\n",
    "      \n",
    "- 不改变卷积层的形状\n",
    "\n",
    "\n",
    "### 4. 池化层\n",
    "\n",
    "- 解决过拟合现象：\n",
    "    - 特征提取，减少卷积核输出的像素特征的数量，减少运算量\n",
    "    - Pooling的方法很多，通常采用最大池化\n",
    "        - max_polling:取池化窗口的最大值\n",
    "        - avg_polling:取池化窗口的平均值\n",
    "        \n",
    "    - 池化层输出大小的计算公式与卷积层的公式计算一样\n",
    "        - 卷积向下取整，池化向上取整。0-100 -> 从下往上\n",
    "        \n",
    "- 池化层的通道数与卷积层的通道数量一样\n",
    "\n",
    "\n",
    "#### 池化API：\n",
    "\n",
    "- tf.nn.max_pool(value, ksize=, strides=, padding=,name=None)\n",
    "\n",
    "    - value:4-D Tensor形状[batch, height, width, channels]\n",
    "    \n",
    "        - channel:并不是原始图片的通道数，而是多少个filter观察\n",
    "        - 100张图片被50个人观察，得出50个32*32的结果 ==> [100,32,32,50]\n",
    "        \n",
    "    - ksize:池化窗口大小，[1, ksize, ksize, 1]，一般为2*2\n",
    "    - strides:步长大小，[1,strides,strides,1]，上下左右的步长，一般都是2\n",
    "    - padding:“SAME”, “VALID”，使用的填充算法的类型，默认使用“SAME”\n",
    "    - \n",
    "\n",
    "### 一般使用现成的神经网络结构\n",
    "    \n",
    "    - resNet，Googlenet\n",
    "    \n",
    "    \n",
    "### 5. Full Connection层\n",
    "\n",
    "- 前面的卷积和池化相当于做特征工程，最后的全连接层在整个卷积神经网络中起到“分类器”的作用。\n",
    "\n",
    "\n",
    "\n",
    "## 案例：利用cnn对手写数字识别:\n",
    "\n",
    "确定网络结构和参数：\n",
    "两层“隐层”和一层输出层\n",
    "\n",
    "第一层\n",
    "    卷积：32个filter、大小5x5、strides=1、padding=\"SAME\"\n",
    "    激活：Relu\n",
    "    池化：大小 2x2、strides = 2\n",
    "第二层\n",
    "    卷积：64个filter、大小5x5、strides=1、padding=\"SAME\"\n",
    "    激活：Relu\n",
    "    池化：大小 2x2、strides = 2\n",
    "全连接层\n",
    "    \n",
    "    输出10个类别0-9\n",
    "重点：计算每层数据的变化\n",
    "\n",
    "\n",
    "\n",
    "1. 准备数据，使用tf.nn.conv2d(input, filter, strides=, padding=, name=None)\n",
    "    - 第一层\n",
    "        - n张黑白图片，每张图片28*28\n",
    "        - input = [n，28，28，1]\n",
    "        - filter = 指定过滤器的权重数量 [5,5,1,32]\n",
    "        - 偏置数量 = [32]\n",
    "        - 卷积输出 = [n，28，28，32]\n",
    "        \n",
    "        - 激活输出 = [n，28，28，32]\n",
    "        \n",
    "        - 池化输出 = [n，14，14，32]\n",
    "        \n",
    "    - 第二层（第一层输出的channel就是第二层的in_channel）\n",
    "        - 输入 = [n批次，14，14，32]\n",
    "        - 权重数量 = [5,5,32,64]\n",
    "        - 偏置 = [64]\n",
    "        - 卷积输出 = [n，14，14，64]\n",
    "        - 激活输出 = [n，14，14，64]\n",
    "        - 池化输出 = [n，7，7，64]\n",
    "        - 每个样本变成[7,7,64]个特征，n个样本的特征值为[n,7,7,64]\n",
    "        \n",
    "    - 第三层\n",
    "        - 输入4维[n，7，7，64] -> 转化2维  [n，7*7*64]\n",
    "          \n",
    "        - 输出[n, 10] = [n，7*7*64] * [7*7*64个特征,10个输出结果] + bias[10]\n",
    "        - \n",
    "        \n",
    "        - 权重数量[7*7*64个特征,10个输出结果]\n",
    "        \n",
    "        - 偏置[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3] [2 2 2 2] [[2 2 2]\n",
      " [2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "# tf.argmax()总结\n",
    "# tf.argmax()就是np.argmax()\n",
    "# 当参数为0时，表示：x轴方向，每【列】的最大值\n",
    "# 当参数为1时，表示：y轴方向，每【行】的最大值\n",
    "# 当参数为2时，此时是一个3维数组，其结果与参数为1的结果一样\n",
    "\n",
    "# 输出为对应最大值的角标\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([[1,2,3],\n",
    "              [4,5,6],\n",
    "              [7,8,9],\n",
    "              [10,11,12]])\n",
    "con_0 = np.argmax(x,0)\n",
    "con_1 = np.argmax(x,1)\n",
    "\n",
    "y = np.array([ [[1,2,3],\n",
    "               [4,5,6], \n",
    "               [7,8,9]],\n",
    "               [[10,11,12],\n",
    "                [13,14,15],\n",
    "                [16,17,18]]])\n",
    "con_2 = np.argmax(y,2)\n",
    "print(con_0,con_1,con_2)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce_all 判断行或列，是否全是true 或false\n",
    "# 参数1和0与argmax一样\n",
    "x = tf.constant([[True,True],[False,False]])\n",
    "tf.reduce_all(x)    # False\n",
    "tf.reduce_all(x,0)  # [False,False]\n",
    "\n",
    "# 判断每行是否全为True/False\n",
    "tf.reduce_all(x,1)  # [True,False]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
